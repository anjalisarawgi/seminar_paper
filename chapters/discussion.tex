Building on the findings presented in ``The Fragility of Sparsity", our experiments focused on how different normalization methods and different dimensional settings affect the performance and stability of sparsity-based estimators. \\
\\
Our experiments confirmed the significant sensitivity of SBEs to normalization techniques, especially as the dimensionality of the data increases. As the number of predictors $p$ approaches or exceeds the number of observations $n$, we observed that the estimates of SBEs become unstable, as reflected in increased standard errors and inconsistent variable selection. These findings agree with the conclusions of the original paper and highlight the fragility of sparsity assumption under certain conditions. 
% The variability in the number of variables selected by the lasso under different normalization methods further complicates the interpretation of these models, as different normalisations may lead to different sets of `important' predictors.
\\
\\
The results of the Hausman and Residual tests further emphasized this fragility. The Hausman test in our experiments indicated significant differences between OLS and SBEs when $p \approx n$. This suggested that the sparsity assumption may not be applicable in these scenarios. However, the residual test demonstrated that, in certain instances, the residual sum of squares (RSS) from SBEs differed greatly from that of OLS. This indicated that the sparsity assumption could still be valid under specific circumstances. However, these opposite results from the Hausman and the residual test are unclear. A further analysis may be required for a better explanation of why this is true. \\
\\
Further, our experiments for evaluating the effect of normalization choices on the predictive performance of SBEs suggested that in a low dimensional scenario the prediction accuracy, measured by MSE may not be drastically affected by these normalization choices. Additionally, R-squared values remained consistent across different normalization methods, indicating that the overall explainability power of the models did not change significantly with different preprocessing choices. However, in higher-dimensional cases, we observed a different behavior. The MSE fluctuated drastically due to seemingly minor choices, such as the selection of the reference category. This sensitivity raises concerns about the potential fragility of SBEs even in the context of machine learning. Nevertheless, these findings need to be validated across different datasets and contexts.\\
\\
We also observed that preprocessing the dataset is crucial for ensuring the stability of the estimators. For instance, in the Communities and Crime dataset, preprocessing steps such as outlier detection and the application of sparsity-based estimators were essential for obtaining valid results. Without these steps, both SBEs and OLS estimators produced highly unstable coefficients. This highlights the importance of careful prepossessing in real-world datasets, as they can have a significant impact on the estimator's performance.\\
\\
\textbf{Critical Discussion}
\\
\textbf{(A) Choice of Regularization parameter for Lasso:}
In these experiments, the choice of hyperparameters, particularly the regularization parameter (alpha) in Lasso proved to be critical. We evaluated two approaches: using a constant alpha and using a properly tuned alpha through a Python library. \\
\\
For instance, when we applied different alpha values for the same study to evaluate the fragility of sparsity (refer: Table 1 and Table 2) we observed significantly different outcomes. We observed that a properly tuned alpha significantly stabilized our estimates. This indicates that varying alpha values can substantially impact both variable selection and the stability of our estimates.\\
\\
Additionally, in another experiment, different values of alpha also led to significantly different results. In our experiments on the Communities in Crime dataset, a different choice of alpha gave us opposite results in the sparsity tests. For more details on this experiment, please refer to Appendix~\ref{sec:appendix-A.1}.\\
\\
Thereby, the choice of the regularization parameter for lasso should be carefully considered and more analysis has to be done to check if the right choice of the regularization parameter can significantly reduce the problems of sparsity. \\
\\
\textbf{(B) Problems with artificially increasing the feature dimensions:}\\
In our experiments, as mentioned in the results of Table 3, we noticed that one possible reason affecting the fragility of these estimates can be because we increase the features artificially. This increase of features aligns to what the authors implemented in the original study as well and we followed the same. 
\\
\\
It may be possible that the results are different for such artifically increased features because it may make our datasets redundant. To evaluate this if this is true, these experiments should further be done in naturally high-dimensional datasets. This will help us understand if there are any problems in the methodology of these experunebts itself. \\
\\
\textbf{Limitations:} A limitation of these experiments is that they focus on specific datasets, normalization choices, and models. This focus may limit the generalizability of these tests. While the choices of these datasets were aligned with the original paper's methodology, they may not fully encompass the range of scenarios and complexities that can arise in different datasets. Further validation of these results is required through additional experiments across diverse datasets and contexts, as each dataset may present unique challenges that could affect the applicability in the future. \\
\\
One particular challenge in our study was interpreting the results of SBE estimates and coefficients for the Communities and Crime dataset. This dataset was not originally designed for causal inference. Thereby, while checking for the causal effect between the treatment and the outcome, the coefficients were particularly small, making the interpretations challenging.\\
\\
Given the observed fragility of SBEs, further research is needed to explore the robustness of SBEs along with their impact on machine learning. We need to see if these issues affect how well models work, and how much they can affect the final result. The impact of hyperparameter choices is also important to be analyzed and the preprocessing and the artificially set up dataset should also be taken into account. It is also important to develop more reliable versions of these estimators that are versions of these estimators that are less affected by normalization choices. Alternative regularisation techniques that do not rely on sparsity could be more reliable for high-dimensional analysis.

