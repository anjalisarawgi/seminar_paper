Building on the findings presented in "The Fragility of Sparsity", our experiments focused on how different normalization methods and different dimensional settings affect the performance and stability of sparsity-based estimators. \\
\\
Our experiments confirmed the significant sensitivity of SBEs to normalization techniques, especially as the dimensionality of the data increases. As the number of predictors $p$ approaches or exceeds the number of observations $n$, we observed that the estimates of SBEs become unstable, as reflected in increased standard errors and inconsistent variable selection. These findings agree with the conclusions of the original paper and highlight the fragility of sparsity assumption under certain conditions. The variability in the number of variables selected by the lasso under different normalization methods further complicates the interpretation of these models, as different normalisation's may lead to different sets of `important' predictors. \\
\\
The results of the Hausman and Residual tests further emphasized this fragility. The Hausman test in our experiments indicated significant differences between OLS and SBEs when $p \approx n$. This suggested that the sparsity assumption may not be applicable in these scenarios. However, the residual test demonstrated that, in certain instances, the residual sum of squares (RSS) from SBEs was not very different from that of OLS. This indicated that the sparsity assumption could still be valid under specific circumstances. However, these opposite results from the Hausman and the residual test is unclear. A further analysis may be required for a better explaination for why this is true. \\
\\
Further, our experiments for evaluating the effect of normalization choices on the predictive performance of SBEs suggeste that prediction accuracy, measured by MSE in this case, may not be drastically affected by these choices. However, R-squaed values remained consistent across different normalization methods, indicating that the overall explainability power of the models did not change significantly with different preprocessing choices. This sugeests that while normalization may impact the specific variables selected by the model (as seen in Table 1, it might not drastically change the model's predictive capability. Nevertheless, these findings should be validated across different datasets and contexts. \\
\\
We also observed that preprocessing the dataset is crucial for ensuring the stability of the estimators. For instance, in the Communities and Crime dataset, preprocessing steps such as outlier detection and the application of sparsity-based estimators were essential for obtaining valid results. Without these steps, both SBEs and OLS estimators produced highly unstable coefficients. This highlights the importance and careful prepossessing in real-world datasets (which is usually preprocessed in practice), as they can significantly impact the estimators performance.  \\\
\\
The tests for understanding the effect of normalization choices in the predictive performance of the models, suggested that the prediction accuracy of SBEs might not be drrastically affected. However, the r-squared values remained onsistent, which indicated that the overall explanatory power of the models did not vary significantly with different preprocessing choices. However, this needs to be studied over different datasets with different contexts too.\\
\\
Another important consideration is the choice of hyperparameters, such as the regularization parameter (alpha) in Lasso. Different values of alpha can lead to significantly different results, affecting both variable selection and the stability of the estimates. For example, in our experiments on Communities in Crime dataset, a different choice of alpha did not only affect the coefficients, but also gave us opposite results in the sparsity tests. Here, one alpha value accepted null hypothesis for all normalization choices, whereas, for the next cross validated alpha value, it rejected the null hypothesis for all (appendix). Therefore, hyperparameter selection must be done careuflly to ensure the validity of the results.\\
\\
\textbf{Limitations:} A limitation of these experiments is that it focuses on specific datasets, normalization choices, and models. This focus may limit the generalizability of these tests. While the choices of these datasets were aligned to the original paper's methodology, they may not fully encompass the range of scenarios and complexities that can arrise in different datasets. Further validation of these results is required through additional experiments across diverse datasets and contexts, as each dataset may present unique challenges that could affect the applicability in the future. \\
\\
One particular challenge in our study was interpretting the results of SBE estimates and coefficients for the Communities and Crime dataset. This dataset was not originally designed for causal inference. Thereby, while checking for the causal effect between the treatment and the outcome, the coefficients were particularly small, making the interpretations challenging.\\
\\
Given the observed fragility of SBEs, further research is needed to explore the robustness of SBEs and their impact on machine learning. We need to see if these issues affect how well models work, and how much they can affect the final result. It is also important to develop more reliable versions of these estimators that are versions of these estimators that are less affected by normalisation choices. Alternative regularisation techniques that do not rely on sparsity could be more reliable for high dimensional analysis.
