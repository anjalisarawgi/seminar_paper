Building on the findings presented in \textit{``The Fragility of Sparsity"}, our experiments focused on how different normalization methods and different dimensional settings affect the performance and stability of sparsity-based estimators (SBEs). \\
\\
Our experiments confirmed the significant sensitivity of SBEs to normalization techniques, especially as the dimensionality of the data increases. As the number of predictors $p$ approaches or exceeds the number of observations $n$, we observed that the estimates of SBEs become unstable, as reflected in increased standard errors and inconsistent variable selection. These findings agree with the conclusions of the original paper and highlight the fragility of sparsity assumption under certain conditions. 
% The variability in the number of variables selected by the lasso under different normalization methods further complicates the interpretation of these models, as different normalisations may lead to different sets of `important' predictors.
\\
\\
The results of the Hausman and Residual tests further emphasized this fragility. The Hausman test in our experiments indicated significant differences between OLS and SBEs when $p \approx n$. This suggested that the sparsity assumption may not be applicable in these scenarios. However, the residual test supported the sparsity assumption, as it found no significant difference between SBEs and OLS. It suggested that the sparsity assumption could still be valid under specific circumstances. However, these opposite results from the Hausman and the Residual test are unclear, and further analysis may be required for a better explanation of why this is true. \\
\\
Our experiments for evaluating the effect of normalization choices on the predictive performance of SBEs revealed interesting patterns. It suggested that in a low dimensional scenario the prediction accuracy, measured by MSE may not be drastically affected by these normalization choices. Additionally, R-squared values remained consistent across different normalization methods, indicating that the overall explainability power of the models did not change significantly with different preprocessing choices. However, in higher-dimensional cases, we observed a different behavior. The MSE fluctuated drastically due to seemingly minor choices, such as the selection of the reference category. This sensitivity raises concerns about the potential fragility of SBEs even in the context of machine learning. Nevertheless, these findings need to be validated across different datasets and contexts.\\
\\
We also found that preprocessing the dataset is crucial for ensuring the stability of the estimators. For instance, in the Communities and Crime dataset, preprocessing steps such as outlier detection and multi-collinearity checks were essential for obtaining valid results. Without these steps, both SBEs and OLS estimators produced highly unstable coefficients. This highlights the importance of careful prepossessing in real-world datasets, to avoid misleading results.\\
\\
{\large\textbf{Critical Discussion}}\\
\\
\textbf{(A) Choice of Regularization parameter for Lasso:}
In these experiments, the choice of hyperparameters, particularly the regularization parameter (alpha) in Lasso, proved to be critical. We evaluated two approaches: using a constant alpha and using a properly tuned alpha through a Python library. \\
\\
For example, when we applied different alpha values for the same study to evaluate the fragility of sparsity (refer to Table 1 and Table 2) we observed significantly different outcomes. We observed that a properly tuned alpha significantly stabilized our estimates. This indicated that varying alpha values can substantially impact both variable selection and the stability of our estimates.\\
\\
In another experiment with the Communities and Crime dataset, different values of alpha also led to significantly different results in the sparsity tests, sometimes even giving opposite conclusions. For more details on this experiment, please refer to Appendix~\ref{sec:appendix-A.1}.\\
\\
Thereby, the choice of the regularization parameter for lasso should be carefully considered and more analysis has to be done to check if the right choice of the regularization parameter can significantly reduce the problems of sparsity. \\
\\
\textbf{(B) Problems with artificially increasing the feature dimensions:}
In our experiments, as discussed in the results of Table 3, we noticed that artificially increasing the number of features could contribute to the fragility of the estimates. This is because such transformations would make the dataset highly redundant possibly causing other complexities. This artificial increase in the number of features is similar to the methodology implemented in the original paper, which we followed and attempted to replicate. \\
\\
However, it is possible that other factors may have influenced the results. To evaluate whether the artificially increased features add to the fragility of the estimates, further experiments need to be conducted, particularly on naturally high-dimensional datasets. \\
\\
{\large\textbf{Limitations}}\\
\\
A limitation of these experiments is their focus on specific datasets, normalization choices, and models. While the choices of these datasets were aligned with the original paper's methodology, they may limit the generalizability of the findings. Further validation of these results is required through additional experiments across diverse datasets and contexts, as each dataset may have unique challenges that could affect the applicability of SBEs. \\
\\
One particular challenge in our study was interpreting the results of SBE estimates and coefficients for the Communities and Crime dataset. This dataset was not originally designed for causal inference. Thereby, while checking for the causal effect between the treatment and the outcome, the coefficients were particularly small, making the interpretations challenging.