Additional material goes here
% \bibliographystyle{plain}
% \begin{thebibliography}{9}

% \bibitem{causal_inference} 
% Causal inference. 
% \textit{Wikipedia, The Free Encyclopedia}. 
% Available: \url{https://en.wikipedia.org/wiki/Causal_inference#:~:text=Causal%20inference%20is%20the%20process,component%20of%20a%20larger%20system}. 
% Accessed: [August 2024].

% \bibitem

% \end{thebibliography}

\begin{table}[h!]
\renewcommand{\arraystretch}{1.5}
\centering
\begin{tabular}{||l l||} 
 \hline
 \hline
 $p$ & Standard Error(OLS) \\ [0.5ex] 
 \hline\hline
 10 & 641.132017755161 \\ 
 54 & 694.8235041339607  \\
 99 & 751.549120310562 \\ 
 188 & 792.3967783851008 \\ 
 321 & 827.0268217222575 \\[1ex] 
 \hline
\end{tabular}
\caption{Standard errors of OLS as p increases -- becomes bad }
\label{table:1}
\end{table}

\begin{table}[h!]
\renewcommand{\arraystretch}{1.5}
\centering
\begin{tabular}{||l l l l l l||} 
 \hline
 \hline
 Case & Drop1 & Drop2 & Drop3 & Demean & Median \\ [0.5ex] 
 \hline\hline 
 % \multicolumn{6}{||c||}{Treatment Coefficient Standard Error} \\ [0.5ex]
 % Original $p$: & 0.0002 & 0.0002 & 0.0002 & 0.0002 & 0.0002 \\ 
 % $p$ close to $n$: & 0.0006 & 0.0006 & 0.0006 & 0.0006 & 0.0006 \\
 % $p$ higher than $n$: & 0.0010 & 0.0008 & 0.0008 & 0.0008 & - \\ [1ex] 
 % \hline
 \multicolumn{6}{||c||}{Number of Variables Selected by lasso} \\ [0.5ex]
 Original $p$ ($p = 159$) & 65 & 67 & 66 & 65 & 65 \\ 
 $p$ close to $n$ ($p = 1583$) & 1283 & 1269 & 1270 & 1272 & 1284 \\
 $p$ higher than $n$ ($p= 2240$) & 1441 & 1439 & 1430 & 1424 & 1449 \\ [1ex] 
 \hline
\end{tabular}
\caption{Coefficients of Treatment and Standard Error for Crime and communities Dataset.  \\
Alpha for the lasso (which from scikit learn is 10 we didnt use the default because 1.0 is too low and lassocv makes selected lasso variables as 1 or 0) is taken the default value because remember that the study is based on software defaults. This will vary differently for different choices of lasso. Here, our n = 1892. The features are increased by using several feature transform methods like polynomial features, noise additions etc.  }
\textcolor{red}{we avoided putting the coefficients because its super small and it would be hard to interpret it!}
\label{table:1}
\end{table}

\begin{table}[h!]
\renewcommand{\arraystretch}{1.5}
\centering
\begin{tabular}{||l l l l l||} 
 \hline
 \hline
 Test & Drop1 & Drop2 &  Demean & Median \\ [0.5ex] 
 \hline \hline
 \multicolumn{5}{||c||}{alpha = 10} \\ [0.5ex]
 Hausman Test & 0.22052 & 0.47046 & 0.3609 & 0.2580 \\ 
 Residual Test & 1.0 & 1.0 & 1.0 & 1.0 \\
 \hline
 \multicolumn{5}{||c||}{Lasso CV} \\ [0.5ex]
 Hausman Test & 5.84e-13 & 5.84e-13 & 3.66e-13 & 1.23e-13 \\ 
 Residual Test: & 1.11e-16 & 1.11e-16  & 1.11e-16 & 1.11e-16 \\ [1ex] 
 \hline \hline
\end{tabular}
\caption{crime DATASET + ORIGINAL CASE: p value of the two tests on both the datasets. Note that we used the cross validated lasso for lalonde dataset\\
\\
for lalonde we accept the null hypothesis and say that the difference has no difference and say the sparsity assumptions hold (makes sense)

for communities and crime we reject the null hypothesis and say the sparsity assumption does not hold (makes sense too cause we want all variables) 
}
\textcolor{red}{we cannot do a more than n case because ols benchmark is not valid}

\label{table:1}
\end{table}
