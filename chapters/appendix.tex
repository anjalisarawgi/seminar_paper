% \bibliographystyle{plain}
% \begin{thebibliography}{9}

% \bibitem{causal_inference} 
% Causal inference. 
% \textit{Wikipedia, The Free Encyclopedia}. 
% Available: \url{https://en.wikipedia.org/wiki/Causal_inference#:~:text=Causal%20inference%20is%20the%20process,component%20of%20a%20larger%20system}. 
% Accessed: [August 2024].

% \bibitem

% \end{thebibliography}

\subsection{Evaluating OLS behaviour}
\begin{table}[h!]
\renewcommand{\arraystretch}{1.5}
\centering
\begin{tabular}{||l l||} 
 \hline
 \hline
 $p$ & Standard Error(OLS) \\ [0.5ex] 
 \hline\hline
 10 & 641.132017755161 \\ 
 54 & 694.8235041339607  \\
 99 & 751.549120310562 \\ 
 188 & 792.3967783851008 \\ 
 321 & 827.0268217222575 \\[1ex] 
 \hline
\end{tabular}
\caption{\textit{Standard errors of OLS Estimator with increasing number of Predictiors. Dataset used: Lalonde Dataset}}
\label{table:1}
\end{table}
This table shows the standard error odf the Ordinary Least Squares (OLS) estimator as the number of predictors ($p$) increases. We notice that as $p$ gets larger, the standard error increases, suggesting that OLS estimator is less reliable and stable i high-dimensional settings. This result confirms the statement that OLS gets noisy for increasing number of dimensions. 

\subsection{Evaluating the number of variables selected by lasso (Communities and Crime Dataset)}

\begin{table}[h!]
\renewcommand{\arraystretch}{1.5}
\centering
\begin{tabular}{||l l l l l l||} 
 \hline
 \hline
 Case & Drop1 & Drop2 & Drop3 & Demean & Median \\ [0.5ex] 
 \hline\hline 
 % \multicolumn{6}{||c||}{Treatment Coefficient Standard Error} \\ [0.5ex]
 % Original $p$: & 0.0002 & 0.0002 & 0.0002 & 0.0002 & 0.0002 \\ 
 % $p$ close to $n$: & 0.0006 & 0.0006 & 0.0006 & 0.0006 & 0.0006 \\
 % $p$ higher than $n$: & 0.0010 & 0.0008 & 0.0008 & 0.0008 & - \\ [1ex] 
 % \hline
 % \multicolumn{6}{||c||}{Number of Variables Selected by lasso} \\ [0.5ex]
 Original $p$ ($p = 159$) & 65 & 67 & 66 & 65 & 65 \\ 
 $p$ close to $n$ ($p = 1583$) & 1283 & 1269 & 1270 & 1272 & 1284 \\
 $p$ higher than $n$ ($p= 2240$) & 1441 & 1439 & 1430 & 1424 & 1449 \\ [1ex] 
 \hline
\end{tabular}
\caption{\textit{Number of variables selected by Lasso for Different Normalization Methods in the Communities and Crimes Dataset. Here, n = 1,892 }}
\label{table:1}
\end{table}

\textit{Note:} The number of predictors ($p$) is increased through feature transformation techniques which include, polynomial expansions, interaction terms and noise additions. We did not include the coefficient values as they were very small, making them challenge to interpret meaningfully. \\
\\
The table shows a similar behaviour to that of the Lalonde dataset results. We can see that for the original case, where the number of predictors ($p$) is much smaller than the number of observatios ($n$), the number of variables selected is similar across the different normalization methods. However, as the number of dimensions $p$ increases, there is more fluctuations in the number of variables selected across different normalizations, highlighting its sensitivity. 
\textcolor{red}{you used the dataset without checks - see if you can make it for datasets with checks}




\subsection{Evaluating for different regularization parameter ($\alpha$) values in Lasso}
\begin{table}[h!]
\renewcommand{\arraystretch}{1.5}
\centering
\begin{tabular}{||l l l l l||} 
 \hline
 \hline
 Test & Drop1 & Drop2 &  Demean & Median \\ [0.5ex] 
 \hline \hline
 \multicolumn{5}{||c||}{$\alpha$ = 10} \\ [0.5ex]
 Hausman Test & 0.22052 & 0.47046 & 0.3609 & 0.2580 \\ 
 Residual Test & 1.0 & 1.0 & 1.0 & 1.0 \\
 \hline
 \multicolumn{5}{||c||}{Lasso CV} \\ [0.5ex]
 Hausman Test & 5.84e-13 & 5.84e-13 & 3.66e-13 & 1.23e-13 \\ 
 Residual Test: & 1.11e-16 & 1.11e-16  & 1.11e-16 & 1.11e-16 \\ [1ex] 
 \hline \hline
\end{tabular}
\caption{\textit{P-values from Hausman and Residual Tests for different normalization strategies and alpha values. Dataset used: Communities and Crime Dataset}}
\label{table:1}
\end{table}


Here, for the first case ($\alpha = 10$), the results indicate that we accept the null hypothesis for both cases at 10\% level of significance. However, for a alpha value which is cross validated through the LassoCV method in the Scikit-learn package, the tests indicate the opposite results. In the second case, we would reject the null hypothesis for both the tests, across all normalizations indicating the sparsity assumption does not hold.\\ 
\\
This table highlights how different choices of lasso values can significantly affect our results and is also possible that our results might be reversed entirely. \\
\\
Note: The lassoCV method chooses a extremely high alpha value than the alpha value 10. This is likely the reason for the stark contrast of the results. 