\textcolor{brown}{Part 1: Checking if the results are fragile for different normalizations: }

In the Lalonde dataset looks at how a job training program affects people's earnings. The dataset has information on 445 people. It has 12 variables, including data from the treatment and control groups. This allows us for testing of causal inference methods. Our variables are:
\begin{enumerate}
    \item \textbf{Treatment variable:} The "treat" variable in this dataset treatment variable which shows if someone took part in the training program (1 = yes, 0 = no). It allows us to assess if there is an impact of the job training on the outcome variable. 
    \item \textbf{Outcome variable:} "re78" is the outcome variable for this dataset which represents real earnings in 1989. It measures post treatment earnings to evaluate the impact of the job training program. 
    \item \textbf{Control variables:} The control variables for our model from this dataset include: age, educ, black, hisp, married, nodegr, re74, re75, u74, u75. These variables account for demographic factors (age, educ, black, hisp, married, nodegr), pre-treatment earnings (re74, re75) and employment status before the intervention (u74, u75), allowing us to control for the differences that could impact the outcome. 
\end{enumerate}

Note: We use the default software defaults just like the original paper "The Fragility of Sparsity". So for lasso, we use the default alpha value which is 1.0 in the scikit-learn package. 

\begin{table}[h!]
\renewcommand{\arraystretch}{1.5}
\centering
\begin{tabular}{||l l l l l l||} 
 \hline
 \hline
 Case & Drop1 & Drop2 & Demean & Median & OLS \\ [0.5ex] 
 \hline \hline
 \multicolumn{6}{||c||}{Treatment Coefficient} \\ [0.5ex]
 Original $p$ ($p = 10$) & 1670.71 & 1670.71 & 1691.39 & 1670.71 & 1670.71 \\ 
 $p$ close to $n$ ($p = 366$) & 2584.34 & 2701.57 & 2425.79 & 2608.61 & 2642.21 \\
 $p$ higher than $n$ ($p= 499$) & 2474.55 & 2396.02 & 2431.1 & 2505.07 & - \\ [1ex] 
 \hline
 \multicolumn{6}{||c||}{Treatment Coefficient - Standard Error} \\ [0.5ex]
 Original $p$: & 641.13 & 641.13 & 638.67 & 641.13 & 641.13 \\ 
 % $p$ close to $n$ (p = 188) : & 793.01 & 795.445 & 783.22 & 793.778 & 792.39 \\
 $p$ close to $n$ (p = 366) & 829.40 & 821.09 & 809.31 & 830.15 & 824.34 \\
 $p$ higher than $n$: & 824.44 & 832.19 & 818.07 & 833.46 & - \\ [1ex] 
 \hline
 \multicolumn{6}{||c||}{Number of Variables Selected by lasso} \\ [0.5ex]
 Original $p$ ($p = 10$) & 10 & 10 & 8 & 10 & - \\ 
 $p$ close to $n$ ($p = 366$) & 268 & 271 & 229 & 268 & - \\
 $p$ higher than $n$ ($p= 499$) & 353 & 363 & 306 & 353 & - \\ [1ex] 
 \hline \hline
\end{tabular}
\caption{Coefficients of Treatment and Standard Error for Lalonde Dataset.  \\
Alpha for the lasso (which from scikit learn is 1.0) is taken the default value because remember that the study is based on software defaults. This will vary differently for different choices of lasso. Here, our n = 445. The features are increased by using several feature transform methods like polynomial features, noise additions etc.  }
\textcolor{red}{idk why standard errors for sbes also increase}
\label{table:1}
\end{table}



% \begin{table}[h!]
% \renewcommand{\arraystretch}{1.5}
% \centering
% \begin{tabular}{||l l l l l l||} 
%  \hline
%  \hline
%  Case & Drop1 & Drop2 & Drop3 & Demean & Median \\ [0.5ex] 
%  \hline\hline
%  \multicolumn{6}{||c||}{Treatment Coefficient Standard Error} \\ [0.5ex]
%  Original $p$: & 0.0002 & 0.0002 & 0.0002 & 0.0002 & 0.0002 \\ 
%  $p$ close to $n$: & 0.0006 & 0.0006 & 0.0006 & 0.0006 & 0.0006 \\
%  $p$ higher than $n$: & 0.0010 & 0.0008 & 0.0008 & 0.0008 & - \\ [1ex] 
%  \hline
%  \multicolumn{6}{||c||}{Number of Variables Selected by lasso} \\ [0.5ex]
%  Original $p$ ($p = 159$) & 125 & 125 & 127 & 126 & 124 \\ 
%  $p$ close to $n$ ($p = 1861$) & 1516 & 1515 & 1516 & 1509 & 1521 \\
%  $p$ higher than $n$ ($p= 2240$) & 1858 & 1854 & 1846 & 1860 & 1860 \\ [1ex] 
%  \hline
% \end{tabular}
% \caption{Coefficients of Treatment and Standard Error for Crime and communities Dataset.  \\
% Alpha for the lasso (which from scikit learn is 10 we didnt use the default because 1.0 is too low and lassocv makes selected lasso variables as 1 or 0) is taken the default value because remember that the study is based on software defaults. This will vary differently for different choices of lasso. Here, our n = 1892. The features are increased by using several feature transform methods like polynomial features, noise additions etc.  }
% \label{table:1}
% \end{table}


\begin{table}[h!]
\renewcommand{\arraystretch}{1.5}
\centering
\begin{tabular}{||l l l l l l||} 
 \hline
 \hline
 Case & Drop1 & Drop2 & Drop3 & Demean & Median \\ [0.5ex] 
 \hline\hline 
 % \multicolumn{6}{||c||}{Treatment Coefficient Standard Error} \\ [0.5ex]
 % Original $p$: & 0.0002 & 0.0002 & 0.0002 & 0.0002 & 0.0002 \\ 
 % $p$ close to $n$: & 0.0006 & 0.0006 & 0.0006 & 0.0006 & 0.0006 \\
 % $p$ higher than $n$: & 0.0010 & 0.0008 & 0.0008 & 0.0008 & - \\ [1ex] 
 % \hline
 \multicolumn{6}{||c||}{Number of Variables Selected by lasso} \\ [0.5ex]
 Original $p$ ($p = 159$) & 65 & 67 & 66 & 65 & 65 \\ 
 $p$ close to $n$ ($p = 1583$) & 1283 & 1269 & 1270 & 1272 & 1284 \\
 $p$ higher than $n$ ($p= 2240$) & 1441 & 1439 & 1430 & 1424 & 1449 \\ [1ex] 
 \hline
\end{tabular}
\caption{Coefficients of Treatment and Standard Error for Crime and communities Dataset.  \\
Alpha for the lasso (which from scikit learn is 10 we didnt use the default because 1.0 is too low and lassocv makes selected lasso variables as 1 or 0) is taken the default value because remember that the study is based on software defaults. This will vary differently for different choices of lasso. Here, our n = 1892. The features are increased by using several feature transform methods like polynomial features, noise additions etc.  }
\textcolor{red}{we avoided putting the coefficients because its super small and it would be hard to interpret it!}
\label{table:1}
\end{table}




\begin{table}[h!]
\renewcommand{\arraystretch}{1.5}
\centering
\begin{tabular}{||l l||} 
 \hline
 \hline
 $p$ & Standard Error(OLS) \\ [0.5ex] 
 \hline\hline
 10 & 641.132017755161 \\ 
 54 & 694.8235041339607  \\
 99 & 751.549120310562 \\ 
 188 & 792.3967783851008 \\ 
 321 & 827.0268217222575 \\[1ex] 
 \hline
\end{tabular}
\caption{Standard errors of OLS as p increases -- becomes bad }
\label{table:1}
\end{table}


\begin{table}[h!]
\renewcommand{\arraystretch}{1.5}
\centering
\begin{tabular}{||l l l l l l||} 
 \hline
 \hline
 Case & Drop1 & Drop2 & Drop3 & Demean & Median \\ [0.5ex] 
 \hline \hline
 Mean Squared Error (MSE) & 47791.44 & 47839.11 & 48066.58 & 46633.518 & 46376.036 \\ 
 R-Squared (R2) & 0.8701 & 0.8699 & 0.8693 & 0.8732 & 0.8739 \\
 \hline \hline
\end{tabular}
\caption{Coefficients of Treatment and Standard Error for Lalonde Dataset.  \\
Alpha for the lasso (which from scikit learn is 1.0) is taken the default value because remember that the study is based on software defaults. This will vary differently for different choices of lasso. Here, our n = 445. The features are increased by using several feature transform methods like polynomial features, noise additions etc.  }
\textcolor{red}{idk why standard errors for sbes also increase}
\label{table:1}
\end{table}



\begin{table}[h!]
\renewcommand{\arraystretch}{1.5}
\centering
\small
\resizebox{\textwidth}{!}{%
\begin{tabular}{||l l l l l l l||} 
 \hline
 \hline
 Case & 150 & 500 & 800 & 1000 & 1500 & Full \\ [0.5ex] 
 \hline \hline
 Standard Error (SBE) & 0.015 & 0.014 & 0.013 & 0.012 & 0.011 & 0.010 \\
 % Standard Error (OLS) & 0.015 & 0.8699 & 0.8693 & 0.8732 & 0.8739 & 0.8745 \\
 \hline \hline
\end{tabular}%
}
\caption{Coefficients of Treatment and Standard Error for Lalonde Dataset.  \\
Alpha for the lasso (which from scikit learn is 1.0) is taken the default value because remember that the study is based on software defaults. This will vary differently for different choices of lasso. Here, our n = 445. The features are increased by using several feature transform methods like polynomial features, noise additions etc.  }
\label{table:1}
\end{table}

