To understand the behaviour of sparsity based estimators in a machine learning context, I conducted two experiments. These tests aimed to test how (i) standard errors behave for varying number of observations ($n$) and (ii) to evaluate whether the different normalization methods affect the predictive power of the models measured by mean squared error (MSE). \\
\\
For these tests, we use the \textbf{Communities and Crime Dataset}. This dataset provides us with a large number of observations which is useful to evaluate how the results change for different sample sizes.\\
\\
Note: While communities and crime dataset consists of approximately 1,900 obser-
vations, which is smaller than typical datasets used in machine learning evaluations,
it was chosen deliberately to test the sparsity assumption in high-dimensional cases.
For testing the findings of the main paper, it often requires us for our number of predictors to be extremely high, approaching the number of observations n itself. Using a much larger dataset, such as one with 20,000 observations would necessitate increasing p to be around 10,000 or more. This could result in highly redundant datasets due to the excessive number of artificially added features. The communities and Crime dataset thus serves as an ideal compromise, providing sufficient observations to evaluate the prediction performance without introducing undue redundancy.\\
\\
\textbf{(A) Testing for varying number of observations ($n$)}\\
To examine the effect of varying sample sizes on the fragility of SBEs, we
performed experiments using different sample sizes from the Communities
and Crime dataset: the full dataset, and subsets of 150, 500, 800, 1000 and 1500. This enables us to evaluate whether the results where consistent
across different sample sizes and provided insights into the robustness of SBEs for
varying dataset sizes. \\
\begin{table}[h!]
\renewcommand{\arraystretch}{1.5}
\centering
\small
\resizebox{\textwidth}{!}{%
\begin{tabular}{||l l l l l l l||} 
 \hline
 \hline
 Number of Observations (n) & 150 & 500 & 800 & 1000 & 1500 & Full \\ [0.5ex] 
 \hline \hline
 Standard Error (SBE) & 0.015 & 0.014 & 0.013 & 0.012 & 0.011 & 0.010 \\
 % Standard Error (OLS) & 0.015 & 0.8699 & 0.8693 & 0.8732 & 0.8739 & 0.8745 \\
 \hline \hline
\end{tabular}%
}
\caption{Treatment Standard Error for Communities and crime dataset}
\label{table:1}
\end{table}

\textbf{Interpretation:} These results are for dropping the first reference category along with no offset normalization for the categorical variables. The table shows that as we decrease the number of observations $n$, the standard error also continues to increase. This pattern indicates that larger sample sizes help stabilize the estimates from sparsity based estimators (SBEs), reducing their fragility. Smaller sample sizes on the other hand are associated with larger standard errors which indicates greater variability and less reliable estimates. \\
\\
\textit{Note:} The communities and crime dataset is not a standard causal inference dataset. When testing with "pop" (population) as the treatment variable, we observed that the coefficients for the treatment are very small. This outcome suggests that there is a minimal causal effect of the treatment on the outcome variable. However, this should not affect our findings on the robustness of SBEs because our focus it to study the behaviour of the standard errors rather than the causal effect size itself.\\
\\
\textbf{(B) Predictive Performance Analysis }\\
This experiment is aimed to evaluate the impact of SBEs within a machine learning context, where the primary focus is the prediction performance of models. For classification tasks, prediction performance can be assessed using metrics such as accuracy, while for regression tasks, metrics such as Mean Squared Error (MSE) or R-Squared are usually used. Our experiments focused on these predictive measures to evaluate if fragile estimates also impact the overall MSE and R squared of the model.
\begin{table}[h!]
\renewcommand{\arraystretch}{1.5}
\centering
\begin{tabular}{||l l l l l l||} 
 \hline
 \hline
 Case & Drop1 & Drop2 & Drop3 & Demean & Median \\ [0.5ex] 
 \hline \hline
 Mean Squared Error (MSE) & 47791.44 & 47839.11 & 48066.58 & 46633.518 & 46376.036 \\ 
 R-Squared (R2) & 0.8701 & 0.8699 & 0.8693 & 0.8732 & 0.8739 \\
 \hline \hline
\end{tabular}
\caption{Mean squared error and R squared measures for SBE method predictions for Communities and Crime Dataset}
\label{table:1}
\end{table}

\textbf{Interpretation:} 
The mean squared error (MSE) varies across different normalization's for the sparsity based estimates. However, the magnitude of these variations is relatively small which suggests that these differences may not be statistically significant. Further analysis in different scenarios might be required to check if these differences are meaningful. \\
\\
The R squared remains consistent across the different normalization methods. This suggests that explainability of the model as indicated by R-squared does not vary much for different normalization choices. 
