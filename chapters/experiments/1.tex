In the original paper, the authors tested for the case where the number of predictors ($p$) is smaller than but very close to the number of observations ($n$). To understand the robustness of sparsity-based estimators (SBEs) across different dimensions, in these experiments, I considered three cases. Each of these cases represented a different relationship between $p$ and $n$: 
\begin{enumerate}
        \item \textbf{Case 1: Original number of predictors ($p \ll n$)}\\
        In this scenario, the OLS estimator is expected to perform well, as it is typically robust and efficient when p is much smaller than n. The sparsity-based estimator (SBE) should also perform reasonably well. 
        \item \textbf{Case 2: Number of predictors close to the number of observations ($p \approx n$)}\\
        In this setting, the OLS estimator may become unstable. In theory, SBEs should provide more stable and efficient estimates than OLS. However, if the sparsity assumption is fragile, we may notice variability in the estimates for different types of normalizations. 
        \item \textbf{Case 3: Number of predictors is more than the number of observations ($p > n$)}\\
        In such a high-dimensional setting, OLS can no longer be used and SBE becomes a preferred choice. This case allows us to rigorously test the robustness of the sparsity assumption when $p$ is higher than $n$. 
    \end{enumerate}

Just like the original paper, we applied various feature transformations to the dataset to increase the dimensions of the data to the appropriate sizes for the second and third cases. The  transformations included:
\begin{enumerate}
    \item \textbf{Polynomial Features:} Creating higher-degree terms to capture non-linear relationships
    \item \textbf{Interaction Terms:} Generating interaction features between existing predictors for interaction effects
    \item \textbf{Statistical Transformations:} Applying logarithmic and square root transformations to the original features
    \item \textbf{Noise Additions:} Introducing random noise to assess the robustness of SBEs when redundant features are also present
\end{enumerate}\\


\begin{table}[h!]
\renewcommand{\arraystretch}{1.5}
\centering
\begin{tabular}{||l l l l l l||} 
 \hline
 \hline
 Case & Drop1 & Drop2 & Demean & Median & OLS \\ [0.5ex] 
 \hline \hline
 \multicolumn{6}{||c||}{(i) Treatment Coefficient Estimate} \\ [0.5ex]
 Original $p$ ($p = 10$) & 1670.71 & 1670.71 & 1691.39 & 1670.71 & 1670.71 \\ 
 $p$ close to $n$ ($p = 366$) & 2584.34 & 2701.57 & 2425.79 & 2608.61 & 2642.21 \\
 $p$ higher than $n$ ($p= 499$) & 2474.55 & 2396.02 & 2431.1 & 2505.07 & - \\ [1ex] 
 \hline
 \multicolumn{6}{||c||}{(ii)Treatment Coefficient Standard Error} \\ [0.5ex]
 Original $p$ ($p$ = 10) & 641.13 & 641.13 & 638.67 & 641.13 & 641.13 \\ 
 % $p$ close to $n$ (p = 188) : & 793.01 & 795.445 & 783.22 & 793.778 & 792.39 \\
 $p$ close to $n$ (p = 366) & 829.40 & 821.09 & 809.31 & 830.15 & 824.34 \\
 $p$ higher than $n$ ($p= 499$)  & 824.44 & 832.19 & 818.07 & 833.46 & - \\ [1ex] 
 \hline
 \multicolumn{6}{||c||}{(iii) Number of Variables Selected by lasso} \\ [0.5ex]
 Original $p$ ($p = 10$) & 10 & 10 & 8 & 10 & - \\ 
 $p$ close to $n$ ($p = 366$) & 268 & 271 & 229 & 268 & - \\
 $p$ higher than $n$ ($p= 499$) & 353 & 363 & 306 & 353 & - \\ [1ex] 
 \hline \hline
\end{tabular}
\caption{\textit{Estimated Treatment Coefficients and Standard Errors using different specifications for the Lalonde Dataset where $n = 445$.} The table reports treatment coefficient estimates, their standard errors, and the number of variables selected by Lasso under the three scenarios. The lasso alpha parameter is set to 1.0, the default in scikit-learn, to maintain consistency with the software defaults mentioned in the original paper. }
% \textcolor{red}{idk why standard errors for sbes also increase}
\label{table:1}
\end{table}

\textbf{Interpretation:}
The table shows how the estimated treatment coefficients, standard errors, and the number of variables selected by Lasso change with different models and normalization. Specifically:\\
\\
\textbf{(A) Treatment Coefficients and Standard Errors:}
\begin{itemize}
    \item When the predictors are small (p = 10), the coefficients and standard errors are stable across all normalizations and their standard errors are relatively low (around 641).
    \item When p gets closer to n (p = 336), both coefficients and standard errors increase, showing greater sensitivity and potential fragility
    \item When the number of predictors exceeds the number of observations (p=499), the treatment coefficients and standard errors continue to show variability across different normalizations. The standard errors remain high, indicating less confidence in the estimates as the dimensionality increases. 

\end{itemize}

\textbf{(B) Number of variables selected by Lasso:}
\begin{itemize}
    \item For a small number of predictors, lasso consistently selects all available variables, regardless of the normalizations
    \item As $p$ approaches $n$, the number of selected variables selected varies significantly for different normalizations i.e. from 229 to as high as 271. This inconsistency reflects that different normalizations also change how many variables the lasso selects. This could highly affect our estimates. A similar behavior is seen in the case when $p>n$. This highlights the instability of the model in high-dimensional settings. 
\end{itemize}

The results in Table 1 are consistent with the findings of the original paper ``The Fragility of Sparsity" as it shows the sensitivity of sparsity-based estimators in high-dimensional settings. Specifically, as the number of predictors increases relative to the number of observations, both the treatment coefficient estimates and the number of variables selected by Lasso estimator demonstrate high variability. This reflects the fragility of SBEs when applied to high-dimensional data. However, the standard errors of the SBEs don't decrease as $p$ gets larger. This contradicts the findings of the original paper on efficiency gains, which states that the standard errors of SBEs are supposed to decrease as the number of dimensions increases. Instead, we observe that the standard errors remain high, suggesting less confidence in the estimates. \\
\\
\textit{However, when these tests were conducted with different Lasso regularization parameter values, the results significantly changed. In this experiment, if we were to change the alpha from 1.0 to use LassoCV, which automatically tunes the hyperparameters for Lasso, we observe very different results.} \\
\\
\textbf{Results with LassoCV: }
When we replace the default Lasso regularization parameter (alpha = 1.0) with LassoCV, our results change significantly. The use of LassoCV leads to more stable results across all three cases, yielding similar treatment coefficient estimates and standard errors. Additionally, the number of variables selected by Lasso also reduces to a more consistent set as seen in Table 2. \\
\begin{table}[h!]
\renewcommand{\arraystretch}{1.5}
\centering
\begin{tabular}{||l l l l l||} 
 \hline
 \hline
 Case & Drop1 & Drop2 & Demean & Median \\ [0.5ex] 
 \hline \hline
 \multicolumn{5}{||c||}{(i) Treatment Coefficient Estimate} \\ [0.5ex]
 Original $p$ ($p = 10$) & 1795.55 & 1795.55 & 1772.60 & 1795.55  \\ 
 $p$ close to $n$ ($p = 366$) & 1791.82 & 1791.82 & 1794.34 & 1791.82 \\
 $p$ more than $n$ (p = 366) & 1791.82 & 1791.82 & 1794.34 & 1791.82  \\
 \hline
 \multicolumn{5}{||c||}{(ii)Treatment Coefficient Standard Error} \\ [0.5ex]
 Original $p$ ($p$ = 10) & 631.20 & 631.20 & 632.60 & 631.20 \\ 
 % $p$ close to $n$ (p = 188) : & 793.01 & 795.445 & 783.22 & 793.778 & 792.39 \\
 $p$ close to $n$ (p = 366) & 633.66 & 633.66& 632.85 & 633.663  \\
 $p$ more than $n$ (p = 366) & 633.66 & 633.66& 632.85 & 633.663 \\
 \hline
 \multicolumn{5}{||c||}{(iii) Number of Variables Selected by Lasso} \\ [0.5ex]
 Original $p$ ($p = 10$) & 1 & 1 & 2 & 1 \\ 
 $p$ close to $n$ ($p = 366$) & 1 & 1 & 0 & 1 \\
 $p$ more than $n$ ($p = 366$) & 1 & 1 & 0 & 1 \\
 \hline \hline
\end{tabular}
\caption{\textit{Estimated Treatment Coefficients and Standard Errors using different specifications for the Lalonde Dataset where $n = 445$.} The table reports treatment coefficient estimates, their standard errors, and the number of variables selected by Lasso under the three scenarios. LassoCV package from a python package scikit-learn is used.}
% \textcolor{red}{idk why standard errors for sbes also increase}
\label{table:1}
\end{table}

Moreover, these results also indicate that once we use a properly tuned Lasso regularization parameter (alpha), the fragility of the SBEs for different normalization methods significantly reduces. The estimators become more stable even in high-dimensional scenarios. This means that Lasso is effectively able to handle high dimensional scenarios with similar standard errors throughout. Thereby we can consider the fact that an appropriate regularization parameter may make a significant difference in our results. 
